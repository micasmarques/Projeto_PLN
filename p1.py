import PyPDF2from nltk.tokenize import word_tokenizefrom nltk.tokenize import sent_tokenizefrom nltk.corpus import stopwordsfrom string import punctuationfrom nltk.probability import FreqDistfrom collections import defaultdictfrom heapq import nlargesttexto = ''f = open(r'C:\Users\micas\Desktop\Faculdade\Mat√©rias\Metodologia\Artigo - Is Compyer science, science (Artigo para resumo).pdf', 'rb')pdf = PyPDF2.PdfFileReader(f)for i in range(pdf.getNumPages()):    texto += pdf.getPage(i).extractText()f.close()pg = texto.split('\n')sentencas = sent_tokenize(texto)palavras = word_tokenize(texto.lower())stopwords = set(stopwords.words('english') + list(punctuation))palavras_sem_stopwords = [palavra for palavra in palavras if palavra not in stopwords]frequencia = FreqDist(palavras_sem_stopwords)sentencas_importantes = defaultdict(int)for i, sentenca in enumerate(sentencas):    for palavra in word_tokenize(sentenca.lower()):        if palavra in frequencia:            sentencas_importantes[i] += frequencia[palavra]idx_sentencas_importantes = nlargest(4, sentencas_importantes, sentencas_importantes.get)# for i in sorted(idx_sentencas_importantes):#     print(sentencas[i])print(pg)